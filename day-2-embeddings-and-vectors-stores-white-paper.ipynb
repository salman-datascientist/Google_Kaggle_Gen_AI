{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"\n%pip install --upgrade gensim\n%pip install --upgrade vertexai","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:37:44.859129Z","iopub.execute_input":"2024-11-27T17:37:44.859476Z","iopub.status.idle":"2024-11-27T17:37:44.876596Z","shell.execute_reply.started":"2024-11-27T17:37:44.859444Z","shell.execute_reply":"2024-11-27T17:37:44.874960Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import google.generativeai as genai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:37:45.821068Z","iopub.execute_input":"2024-11-27T17:37:45.821470Z","iopub.status.idle":"2024-11-27T17:37:46.703071Z","shell.execute_reply.started":"2024-11-27T17:37:45.821436Z","shell.execute_reply":"2024-11-27T17:37:46.701842Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Set up your API key","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:37:46.705273Z","iopub.execute_input":"2024-11-27T17:37:46.705906Z","iopub.status.idle":"2024-11-27T17:37:46.908116Z","shell.execute_reply.started":"2024-11-27T17:37:46.705866Z","shell.execute_reply":"2024-11-27T17:37:46.906652Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:37:59.187134Z","iopub.execute_input":"2024-11-27T17:37:59.187543Z","iopub.status.idle":"2024-11-27T17:38:02.417135Z","shell.execute_reply.started":"2024-11-27T17:37:59.187508Z","shell.execute_reply":"2024-11-27T17:38:02.415822Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Tokenize the input string data\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ndata = [\n    \"The earth is spherical.\",\n    \"The earth is a planet.\",\n    \"I like to eat at a restaurant.\"\n]\n\n#Filter the punctiations, tokenize the words and inex them to integers\ntokenizer = Tokenizer(num_words=15, filters=\"!#$%&()*+,-./:;<=>?[\\\\]^_'{|}~\\t\\n\", lower=True, split=' ')\ntokenizer.fit_on_texts(data)\n\n#Translate each sentence into its word-level IDs, and then one-hot encode those IDs\n\nID_sequences = tokenizer.texts_to_sequences(data)\nbinary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\n\nprint(\"ID dictionary:\\n\", tokenizer.word_index)\nprint(\"\\nID sqeuences:\\n\", ID_sequences)\n\n# One-hot encoding is a binary representation of categorical values where the presence of a word is represented by 1, and its absence by 0. \n#This ensures that the token IDs are treated as categorical values as they are, but often results in a dense vector the size of the vocabulary of the corpus.\n\nprint(\"\\n One-hot encoded squences:\\n\", binary_sequences)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading and plotting GloVe and Word2Vec embeddings in 2D","metadata":{}},{"cell_type":"code","source":"from IPython.core.getipython import get_ipython\n\n# Increase the message rate limit\nget_ipython().run_line_magic('config', \"NotebookApp.iopub_msg_rate_limit=5000\")\nget_ipython().run_line_magic('config', \"NotebookApp.rate_limit_window=10.0\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gensim, matplotlib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\n\n# Set logging to ERROR to suppress warnings/info\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport gensim.downloader as api\nimport pprint\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\ndef t_sne_plt(models,words,seed=23):\n    # Creates a TSNE models and plots for multiple models for the given words\n    \n    plt.figure(figsize=(len(models)*30, len(models)*30))\n    model_ix=0\n    for model in models:\n        labels = []\n        tokens = []\n\n        for word in words:\n            tokens.append(model[word])\n            labels.append(word)\n\n        tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=seed)\n        new_values = tsne_model.fit_transform(np.array(tokens))\n        x = []\n        y = []\n        for value in new_values:\n            x.append(value[0])\n            y.append(value[1])\n    \n        model_ix +=1\n        plt.subplot(10, 10, model_ix)\n\n        for i in range(len(x)):\n            plt.scatter(x[i], y[i])\n            plt.annotate(labels[i],\n                         xy = (x[i], y[i]),\n                         xytext = (5, 2),\n                         textcoords = 'offset points',\n                         ha = 'right',\n                         va = 'bottom' \n                        )\n\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#v2w_model = api.load('word2vec-google-news-300')\nv2w_model = api.load('word2vec-google-news-300', return_path=False, ignore_for_missing=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#glove_model = api.load('glove-twitter-25')\nglove_model = api.load('glove-twitter-25', return_path=False, ignore_for_missing=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Words most similar to 'computer' with word2vec and glove respectively\")\npprint.pprint( v2w_model.most_similar(\"computer\")[:3] )\npprint.pprint( glove_model.most_similar(\"computer\")[:3])\npprint.pprint( \"2d projection of some commoon words of both models\")\nsample_common_words = list(set(v2w_model.index_to_key[100:10000])\n                          &(set(glove_model.index_to_key[100:10000])))[:100]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #tsne_plot([v2w_model, glove_model], sample_common_words)\nt_sne_plt([v2w_model, glove_model], sample_common_words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" Snippet : Self-supervised Training and inference using Doc2Vec on private corpus","metadata":{}},{"cell_type":"code","source":"from gensim.test.utils import common_texts\nfrom gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\nfrom gensim.test.utils import get_tmpfile\n#train model on a sequence of documents tagged with their IDs\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\nmodel = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\n# persist model to disk, and load it to infer on new documents\nmodel_file = get_tmpfile(\"Doc2Vec_v1\")\nmodel.save(model_file)\nmodel = Doc2Vec.load(model_file)  \nmodel.infer_vector([\"human\", \"interface\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating a Keras model using trainable tfhub layer","metadata":{}},{"cell_type":"code","source":"# Can switch the embedding to different embeddings from different modalities on # \ntfhub. Here we use the BERT model as an example.\n tfhub_link = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n class Classifier(tf.keras.Model):\n def __init__(self, num_classes):\n super(Classifier, self).__init__(name=\"prediction\")\n self.encoder = hub.KerasLayer(tfhub_link, trainable=True)\n      self.dropout = tf.keras.layers.Dropout(0.1)\n      self.dense = tf.keras.layers.Dense(num_classes)\n   x\n def call(self, preprocessed_text):\n      encoder_outputs = self.encoder(preprocessed_text)\n      pooled_output = encoder_outputs[\"pooled_output\"]\n = self.dropout(pooled_output)\n   x = self.dense(x)\n return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Using scikit-learn29 and lshashing30 for ANN with LSH, KD/Ball-tree and linear search","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom vertexai.language_models import TextEmbeddingModel\nfrom lshashing import LSHRandom\nimport numpy as np\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\ntest_items= [\n\"The earth is spherical.\",\n\"The earth is a planet.\",\n\"I like to eat at a restaurant.\"]\nquery = \"the shape of earth\"\nembedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])\nembedded_query = np.array(model.get_embeddings([query])[0].values)\n#Naive brute force search\nn_neighbors=2\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='brute').fit(embedded_test_items) \naive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\n#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(embedded_test_items) \nistances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\n#LSH\nlsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)\nlsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)\n#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours\n#ANN retrieved the same ranking of items as brute force in a much scalable manner","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:38:04.744352Z","iopub.execute_input":"2024-11-27T17:38:04.745066Z","iopub.status.idle":"2024-11-27T17:38:24.302681Z","shell.execute_reply.started":"2024-11-27T17:38:04.745028Z","shell.execute_reply":"2024-11-27T17:38:24.300678Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvertexai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextEmbeddingModel\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlshashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSHRandom\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m TextEmbeddingModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextembedding-gecko@004\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lshashing'"],"ename":"ModuleNotFoundError","evalue":"No module named 'lshashing'","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"Indexing and executing ANN search with the FAISS library using HNSW","metadata":{}},{"cell_type":"code","source":" import faiss\n M=32 #creating high degree graph:higher recall for larger index & searching time\n d=768 # dimensions of the vectors/embeddings\n index = faiss.IndexHNSWFlat(d, M)\n index.add(embedded_test_items) #build the index using the embeddings in Snippet 9\n #execute the ANN search\n index.search(np.expand_dims(embedded_query, axis=0), k=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Accuracy/speed tradeoffs for various SOTA ANN search algorithms","metadata":{}},{"cell_type":"code","source":" import tensorflow as tf\n import tensorflow_recommenders as tfrs\n from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n # Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets\n embedded_query = tf.constant((LM_embed(query, \"RETRIEVAL_QUERY\")))\n embedded_docs = [LM_embed(doc, \"RETIREVAL_DOCUMENT\") for doc in searchable_docs]\n embedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs).enumerate().batch(1)\n # Build index from tensorflow dataset and execute ANN search based on dot product metric\n scann = tfrs.layers.factorized_top_k.ScaNN( \n  distance_measure= 'dot_product',\n  num_leaves = 4, #increase for higher number of partitions / latency for increased recall\n  num_leaves_to_search= 2) # increase for higher recall but increased latency\n scann = scann.index_from_dataset(embedded_docs)\n scann(embedded_query, k=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}